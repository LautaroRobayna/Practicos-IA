{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctico 5: Volcano Crossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcano_crossing_env import VolcanoCrossing\n",
    "\n",
    "env = VolcanoCrossing(slip_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jugar manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game():\n",
    "        print(\"Play manually...\")\n",
    "        obs = env.reset()\n",
    "        print(obs)\n",
    "        done = False\n",
    "        step_counter = 0\n",
    "        all_rewards = 0\n",
    "        env.render()\n",
    "\n",
    "        while not done:\n",
    "            action = input(\"Next action: \")\n",
    "            env.check_action(action)\n",
    "            obs, reward, done_env, _ = env.step(action)\n",
    "            print(f'{obs=} {reward=} {done_env=}')\n",
    "            all_rewards += reward\n",
    "            done = done_env\n",
    "            env.render()\n",
    "            step_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play manually...\n",
      "21\n",
      "state: 21 done: False\n",
      "obs=np.str_('31') reward=2 done_env=True\n",
      "state: 31 done: True\n"
     ]
    }
   ],
   "source": [
    "run_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jugar con una policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_south(state) :\n",
    "  return 'S'\n",
    "\n",
    "def policy_north(state) :\n",
    "  return 'N'\n",
    "\n",
    "def policy_east(state) :\n",
    "  return 'E'\n",
    "\n",
    "def policy_west(state) :\n",
    "  return 'W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(policy) :\n",
    "  U = 0\n",
    "  done = False\n",
    "  state = env.reset()\n",
    "  while not done:\n",
    "    state, reward, done, _ = env.step(policy(state))\n",
    "    U += reward\n",
    "  return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(policy_west)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimación de la policy por promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward: -28.09456\n"
     ]
    }
   ],
   "source": [
    "r = 0\n",
    "N = 50_000\n",
    "for i in range(N):\n",
    "    r += run(policy_north)\n",
    "\n",
    "print (f'Average reward: {r/N}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, delta = 0.05, gamma = 0.999):\n",
    "  V = { '11':0, '12':0, '13':0, '14':0, '21':0, '22':0, '23':0, '24':0, '31':0, '32':0, '33':0, '34':0 }\n",
    "  V_new = { '11':0, '12':0, '13':0, '14':0, '21':0, '22':0, '23':0, '24':0, '31':0, '32':0, '33':0, '34':0 }\n",
    "  max_diff = delta + 1\n",
    "\n",
    "  while max_diff > delta:\n",
    "    for s in env.observation_space:\n",
    "      a = policy(s)\n",
    "      q = 0\n",
    "      for s_prime in env.P[s][a].keys():\n",
    "        q += env.P[s][a][s_prime] * (env.R[s][a][s_prime] + gamma * V[s_prime])\n",
    "      V_new[s] = q\n",
    "    max_diff = 0\n",
    "    for s in V.keys():\n",
    "      max_diff = max(max_diff, abs(V[s]-V_new[s]))\n",
    "    V = V_new.copy()\n",
    "  return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': -10.097479836569216, '12': -25.02876509699797, '13': 0.0, '14': 0.0, '21': -5.153966059429463, '22': -21.73443636193771, '23': 0.0, '24': -21.539534697673236, '31': 0.0, '32': -16.790727065526983, '33': -30.83304348111256, '34': -25.988180160178665}\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(policy_south, delta = 0.05, gamma = 0.999)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': -33.67385097676337, '12': -40.458592488691465, '13': 0.0, '14': 0.0, '21': -27.34202796251972, '22': -38.05642247142104, '23': 0.0, '24': 0.13372792866082328, '31': 0.0, '32': -29.944176341195977, '33': -37.83687972706362, '34': -9.342616647035367}\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(policy_north, delta = 0.05, gamma = 0.999)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': -43.9022649270958, '12': -47.30236602463816, '13': 0.0, '14': 0.0, '21': -33.987366121400456, '22': -42.757085499555195, '23': 0.0, '24': -17.621993521940997, '31': 0.0, '32': -25.39581557126935, '33': -28.84154928530964, '34': -23.04484298388258}\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(policy_east, delta = 0.05, gamma = 0.999)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': -17.897256693292853, '12': -24.525782855383536, '13': 0.0, '14': 0.0, '21': -11.63441817939945, '22': -19.239905864923973, '23': 0.0, '24': -30.173214438394798, '31': 0.0, '32': -6.210382176516225, '33': -17.893058456991003, '34': -20.924056781832736}\n"
     ]
    }
   ],
   "source": [
    "V = policy_evaluation(policy_west, delta = 0.05, gamma = 0.999)\n",
    "print(V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "policies = defaultdict(lambda: 'S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(policy, delta = 0.05, gamma=0.99):\n",
    "    \n",
    "    U = policy_evaluation(policy, delta, gamma)\n",
    "    \n",
    "    policy_prime = policy\n",
    "    \n",
    "    for s in env.observation_space:\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for a in env.action_space:\n",
    "            q = 0\n",
    "            for s_prime in env.P[s][a].keys():\n",
    "                q += env.P[s][a][s_prime] * (env.R[s][a][s_prime] + gamma * U[s_prime])\n",
    "            if q > best_value:\n",
    "                best_value = q\n",
    "                best_action = a\n",
    "                policy_prime[s] = best_action\n",
    "                \n",
    "    return policy_prime\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar policy iteration y ver cuál es la mejor acción para cada estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "\n",
    "1. Implementar el algoritmo de Value Iteration para encontrar la policy óptima.\n",
    "2. Obtener la utilidad esperada de esa policy.\n",
    "3. Graficar el valor del estado \"21\" para cada iteración en Value Iteration y Policy Iteration (con una policy inicial arbitraria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, delta_threshold=0.05, gamma=0.999):\n",
    "    pass\n",
    "\n",
    "def q_value(mdp, state, action, V, gamma):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practico5-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
